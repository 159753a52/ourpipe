# GPT 8-Stage 配置（2节点8卡纯流水线并行）
#
# 使用方法:
#   sbatch submit_gpipe_2nodes.sh
# 或交互式:
#   salloc --nodes=2 --gpus-per-node=4
#   bash run_gpipe_2nodes_interactive.sh

model:
  name: gpt
  hidden_size: 512
  num_layers: 16        # 16 层 decoder，分配到 8 个 stage
  num_heads: 16
  sequence_length: 128
  vocab_size: -1        # 从数据集获取
  extra:
    head_size: 32
    ff_low_rank: null
    dropout: 0.4

dataset:
  name: code_search_net
  batch_size: 64
  num_workers: 0
  extra: {}

training:
  learning_rate: 0.0001
  epochs: 2
  num_microbatches: 8   # 增加 micro-batch 数量以提高流水线效率
  gradient_accumulation: 1
  max_iterations: -1
  profile_iteration: 100
  exit_after_profile: true

parallel:
  model_parallel_size: 8  # 8 个流水线 stage（2节点 x 4卡）
  data_parallel_size: 1   # 纯流水线并行，无数据并行
  use_orion_scheduler: false
