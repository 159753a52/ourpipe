# GPT 模型配置 - 8 GPU (2 节点) - Naive 调度器
# 
# 基于 gpt_large_1node.yaml 扩展
# 架构: 2 节点 × 4 GPU = 8 个 stage
# 调度器: Naive (同步阻塞)
#
# 扩展策略：保持每 stage 6 层，增加总层数
#   - 原配置: 4 stages × 6 layers = 24 层
#   - 新配置: 8 stages × 6 layers = 48 层
#
# 显存估算（每 GPU）：
#   - 模型参数 (每 stage): ~150M params × 4 bytes = ~600MB
#   - 优化器状态 (Adam): ~150M × 8 bytes = ~1.2GB
#   - 梯度: ~150M × 4 bytes = ~600MB
#   - 激活值缓存: batch × seq × hidden × layers_per_stage × microbatches × 4 bytes
#                 = 4 × 512 × 1536 × 6 × 8 × 4 ≈ 600MB per microbatch
#   - 通信缓冲区: ~1GB
#   - 总计: ~20-22GB per GPU

model:
  name: gpt
  hidden_size: 1536       # 保持与 1node 相同
  num_layers: 48          # 48 层，每 stage 6 层 (8 stages × 6)
  num_heads: 24           # 1536 / 24 = 64 (head_size)
  sequence_length: 512    # 保持与 1node 相同
  vocab_size: -1          # 从数据集获取
  extra:
    ff_low_rank: null     # 不使用低秩分解
    dropout: 0.1

dataset:
  name: code_search_net
  batch_size: 32          # 保持与 1node 相同
  num_workers: 4
  extra: {}

training:
  learning_rate: 0.0001
  epochs: 2
  num_microbatches: 8     # 保持与 1node 相同
  gradient_accumulation: 1
  max_iterations: -1 
  profile_iteration: 50
  exit_after_profile: true

parallel:
  model_parallel_size: 8  # 8 个 stage (2 节点 × 4 GPU)
  data_parallel_size: 1
  scheduler: naive        # Naive 调度器
  use_orion_scheduler: false
